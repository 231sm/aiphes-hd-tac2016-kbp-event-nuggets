{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "# LSTM v1 - Simple Lstm experiments\n",
    "from sequences.label_dictionary import LabelDictionary\n",
    "dic = LabelDictionary([], start_index=1)\n",
    "\n",
    "dic.add(\"a\")\n",
    "dic.add(\"b\")\n",
    "print dic\n",
    "\n",
    "assert(\"a\" == dic.get_label_name(dic[\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data/\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import readers.tac_prep_corpus as tc\n",
    "from readers.tac_prep_corpus import *\n",
    "\n",
    "print data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tac2014_train = data_dir+\"/clear_data/data_tac2014_train.json\"\n",
    "data_tac2014_eval = data_dir+\"/clear_data/data_tac2014_eval.json\"\n",
    "data_tac2015_train = data_dir+\"/clear_data/data_tac2015_train.json\"\n",
    "data_tac2015_eval = data_dir+\"/clear_data/data_tac2015_eval.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload(readers.tac_prep_corpus)\n",
    "word_counts = TacPrepJsonCorpus.word_counts_from_jsonfiles(json_files = [data_tac2014_train, data_tac2014_eval, data_tac2015_train])\n",
    "\n",
    "# print \"Top 10 tokens:\"\n",
    "# print word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:20459\n",
      "Tokens with freq>=5 : 6813\n"
     ]
    }
   ],
   "source": [
    "# word_counts.sort(key = lambda x:x[1], reverse=True)\n",
    "print \"Vocab size:%s\" % len(word_counts)\n",
    "print \"Tokens with freq>=%s : %s\" % (5, len( [xx for xx in word_counts if xx[1]>=5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'toured', u'Merion', u'dependence', u'Advocacy', u'BEFORE', u'Musical', u'Fastow', u'Coach', u'six-party', u'volumes']\n"
     ]
    }
   ],
   "source": [
    "# clear the vocab\n",
    "vocab = [xx[0] for xx in word_counts]\n",
    "print vocab[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print len([xx for xx in vocab if (\"/\" in xx)or(\"\\\\\" in xx)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_len = 1000 \n",
    "max_nr_sent = 100000\n",
    "max_sent_len=max_sent_len\n",
    "max_nr_sent=max_nr_sent\n",
    "update_vocab=True\n",
    "update_tags=True\n",
    "unknown_word = \"UNKNWN\"\n",
    "unknown_tag = \"O\"\n",
    "mapping_file = None\n",
    "data_x_fieldname = \"tokens\"\n",
    "data_y_fieldname = \"labels_event\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_non_zero_label(test_seq, zero_label=0):\n",
    "    cnt = 0\n",
    "    for item in test_seq:\n",
    "        for lbl in item.y:\n",
    "            if lbl != zero_label:\n",
    "                cnt += 1\n",
    "                # print item.y\n",
    "                break\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use indexes that start from 1 if you use TF dynamic sequences!\n",
    "tag_start_index = 1\n",
    "vocab_start_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tac_corpus = TacPrepJsonCorpus([unknown_word], [unknown_tag], \n",
    "                               tag_start_index=tag_start_index, \n",
    "                               vocab_start_index=vocab_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2015_eval.json\n"
     ]
    }
   ],
   "source": [
    "test_seq, test_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2015_eval], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print tac_corpus.word_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:4164\n",
      "With non zero labels:2646\n",
      "[u'regime', u'<img\\xa0src=\"http://www.amnesty.org/sites/impact.amnesty.org/files/imagecache/news-highlight/brazil-amnesty-law%2025.04.12.JPG\"/>', u'The', u'efforts', u'of', u'federal', u'prosecutors', u'to', u'initiate', u'criminal', u'investigations', u'into', u'past', u'human', u'rights', u'violations', u'marks', u'a', u'crucial', u'moment', u'in', u'Brazil', u\"'s\", u'history', u',', u'said', u'Amnesty', u'International', u'after', u'federal', u'prosecutors', u'in', u'S\\xe3o', u'Paulo', u'charged', u',', u'on', u'24', u'March', u'2012', u',', u'retired', u'Colonel', u'Carlos', u'Alberto', u'Brilhante', u'Ustra', u'and', u'police', u'chief', u'-LRB-', u'delegado', u'-RRB-', u'Dirceu', u'Garvina', u',', u'with', u'the', u'kidnapping', u'of', u'union', u'leader', u'Alu\\xedzio', u'Palhano', u'Pedreira', u'Ferreira', u'in', u'1971', u'.']\n",
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 7, 8, 22, 31, 32, 33, 26, 34, 35, 36, 37, 26, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 26, 52, 53, 54, 6, 55, 56, 57, 58, 59, 60, 22, 61, 62]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"All sents:%s\" % len(test_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(test_seq, zero_label = tag_start_index)\n",
    "print test_seq_meta[0][\"tokens\"]\n",
    "print test_seq[0].x\n",
    "print test_seq[0].y\n",
    "# print [tac_corpus.word_dict[a] for a in test_seq[0].x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2014_train.json\n",
      "readers/../data//clear_data/data_tac2015_train.json\n"
     ]
    }
   ],
   "source": [
    "train_seq, train_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2014_train, data_tac2015_train], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:12573\n",
      "With non zero labels:5367\n"
     ]
    }
   ],
   "source": [
    "print \"All sents:%s\" % len(train_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(train_seq, zero_label = tag_start_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2014_eval.json\n"
     ]
    }
   ],
   "source": [
    "dev_seq, dev_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2014_eval], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:9041\n",
      "With non zero labels:3749\n"
     ]
    }
   ],
   "source": [
    "print \"All sents:%s\" % len(dev_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(dev_seq, zero_label = tag_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'B-EVENT': 2, 'O': 1, u'I-EVENT': 3}\n"
     ]
    }
   ],
   "source": [
    "print train_seq.y_dict\n",
    "\n",
    "# Redo indices\n",
    "train_seq, test_seq, dev_seq = tc.compacify(train_seq, test_seq, dev_seq,\n",
    "                                            default_vocab_dict=[unknown_word], \n",
    "                                            vocab_start_index=vocab_start_index, \n",
    "                                            default_tag_dict=[unknown_tag], \n",
    "                                            tag_start_index=tag_start_index,\n",
    "                                            theano=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get number of words and tags in the corpus\n",
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25584\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for k,v in train_seq.x_dict.iteritems():\n",
    "    #print k,v\n",
    "    if v>nr_words:\n",
    "        print (k,v)\n",
    "        \n",
    "print nr_words\n",
    "print nr_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lstm_v1 as lstm_v1\n",
    "# import theano\n",
    "# import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_words:25584\n",
      "emb_size:50\n",
      "hidden_size:50\n",
      "nr_tags:3\n",
      "lrate:0.0001\n"
     ]
    }
   ],
   "source": [
    "emb_size=50\n",
    "hidden_size=50\n",
    "SEED = 42\n",
    "\n",
    "lrate = 0.0001\n",
    "\n",
    "nr_iterations = 5\n",
    "batch_size = 50\n",
    "print \"nr_words:%s\" % nr_words\n",
    "print \"emb_size:%s\" % emb_size\n",
    "print \"hidden_size:%s\" % hidden_size\n",
    "print \"nr_tags:%s\" % nr_tags\n",
    "print \"lrate:%s\" % lrate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow padding and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# x = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"x\")\n",
    "# y = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"y\")\n",
    "# seq_len = tf.placeholder(dtype=tf.int64, shape=[None], name = \"seq_len\")\n",
    "\n",
    "# batched_data = tf.train.batch(\n",
    "#     tensors=[x, y, seq_len],\n",
    "#     batch_size = batch_size,\n",
    "#     dynamic_pad = True,\n",
    "#     name=\"batch_data\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # data_x = np.array([np.array(a.x) for a in train_seq])\n",
    "# # data_seq_len = np.array([len(a.x) for a in train_seq])\n",
    "# # data_y = np.array([np.array(a.y) for a in train_seq])\n",
    "\n",
    "# data_x = tf.convert_to_tensor([a.x.tolist() for a in train_seq], dtype=tf.int64)\n",
    "# data_seq_len = tf.convert_to_tensor([len(a.x) for a in train_seq], dtype=tf.int64)\n",
    "# data_y = tf.convert_to_tensor([a.y.tolist() for a in train_seq], dtype=tf.int64)\n",
    "\n",
    "# print data_x[:5]\n",
    "# print data_seq_len[:5]\n",
    "# print data_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(batched_data)\n",
    "# print type(data_x)\n",
    "# print type(data_y)\n",
    "# print type(data_seq_len)\n",
    "\n",
    "# #tf.reset_default_graph()\n",
    "\n",
    "# # Run the graph\n",
    "# # tf.contrib.learn takes care of starting the queues for us\n",
    "# # res = sess.run(batched_data[0], batched_data[1], batched_data[2] , feed_dict={x:data_x, y:data_y, seq_len:data_seq_len})\n",
    "# result = tf.contrib.learn.run_n(\n",
    "#     {\n",
    "#         \"batched_data_x\": batched_data[0],\n",
    "#         \"batched_data_y\": batched_data[1],\n",
    "#         \"batched_data_seq_len\": batched_data[2]\n",
    "#     },\n",
    "#     n=1,\n",
    "#     feed_dict={x:data_x, \n",
    "#                seq_len:data_seq_len,\n",
    "#                y:data_y})\n",
    "\n",
    "\n",
    "# Print the result\n",
    "# print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\n",
    "# print(res[0][\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi LSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_number:25584\n",
      "embedding_size:50\n",
      "n_classes:3\n",
      "learning_rate:0.1\n",
      "outputs[-1].shape:Tensor(\"BiLSTM/ReverseSequence:0\", shape=(?, ?, 50), dtype=float64)\n",
      "weights[\"out\"]].shape:<tensorflow.python.ops.variables.Variable object at 0x7fd139b81690>\n",
      "biases[\"out\"].shape:<tensorflow.python.ops.variables.Variable object at 0x7fd13569ae10>\n",
      "input_y_flat:Tensor(\"Reshape_1:0\", shape=(?,), dtype=int64)\n",
      "mask:Tensor(\"Sign:0\", shape=(?,), dtype=float64)\n",
      "probs_flat:Tensor(\"Softmax:0\", shape=(?, 3), dtype=float64)\n",
      "input_y:Tensor(\"input_y:0\", shape=(?, ?), dtype=int64)\n",
      "The model might be okay :D\n"
     ]
    }
   ],
   "source": [
    "embeddings_number = nr_words\n",
    "embedding_size = emb_size\n",
    "n_classes = nr_tags\n",
    "learning_rate=0.1\n",
    "\n",
    "print \"embeddings_number:%s\" % embeddings_number\n",
    "print \"embedding_size:%s\" % embedding_size\n",
    "print \"n_classes:%s\" % n_classes\n",
    "print \"learning_rate:%s\" % learning_rate\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    #'out': tf.Variable(tf.random_uniform([2*hidden_size, n_classes], minval=-1, maxval=1, dtype=tf.float64), name=\"out_w\", dtype=tf.float64)\n",
    "    'out': tf.Variable(tf.truncated_normal([2*hidden_size, n_classes], stddev=0.1, dtype=tf.float64), trainable=True, name=\"out_w\", dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    # 'out': tf.Variable(tf.random_uniform([n_classes], minval=-1, maxval=1, dtype=tf.float64), name=\"out_b\", dtype=tf.float64)\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=0.1, dtype=tf.float64), trainable=True, name=\"out_b\", dtype=tf.float64)\n",
    "}\n",
    "\n",
    "input_x = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"input_x\")\n",
    "input_y = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"input_y\") # this is not onehot!\n",
    "input_seq_len = tf.placeholder(dtype=tf.int64, shape=[None], name = \"input_seq_len\")\n",
    "\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embeddings\"):\n",
    "    embeddings_placeholder = tf.placeholder(tf.float64, shape=[embeddings_number, embedding_size])\n",
    "    # embeddings_tuned = tf.Variable(embeddings_placeholder)\n",
    "    \n",
    "    embeddings_tuned =tf.Variable(tf.truncated_normal([embeddings_number, embedding_size], stddev=0.1, dtype=tf.float64), trainable=True, name=\"out_w\", dtype=tf.float64)\n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings_tuned, input_x)\n",
    "    # embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "\n",
    "def BiLSTM_dynamic(x_embedd, input_seq_len, shared_fw_bw=False,\n",
    "                   use_peepholes=False, cell_clip=None,\n",
    "                   # initializer=None, \n",
    "                   num_proj=None, proj_clip=None,\n",
    "                   num_unit_shards=1, num_proj_shards=1,\n",
    "                   forget_bias=1.0, state_is_tuple=True,\n",
    "                   activation=tf.tanh):\n",
    "    with tf.name_scope(\"BiLSTM\"):\n",
    "        with tf.variable_scope('forward'):\n",
    "            cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True,\n",
    "                                               use_peepholes=use_peepholes, cell_clip=cell_clip,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=42),\n",
    "                                               num_proj=num_proj, proj_clip=proj_clip,\n",
    "                                               num_unit_shards=num_unit_shards, num_proj_shards=num_proj_shards,\n",
    "                                               forget_bias=forget_bias,\n",
    "                                               activation=activation)\n",
    "\n",
    "        with tf.variable_scope('backward'):    \n",
    "            cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True,\n",
    "                                               use_peepholes=use_peepholes, cell_clip=cell_clip,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=24), \n",
    "                                               num_proj=num_proj, proj_clip=proj_clip,\n",
    "                                               num_unit_shards=num_unit_shards, num_proj_shards=num_proj_shards,\n",
    "                                               forget_bias=forget_bias,\n",
    "                                               activation=activation)\n",
    "\n",
    "        outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_fw if shared_fw_bw else cell_bw,\n",
    "            dtype=tf.float64,\n",
    "            sequence_length=input_seq_len,\n",
    "            inputs=x_embedd)\n",
    "\n",
    "    return outputs, states\n",
    "\n",
    "\n",
    "outputs, states = BiLSTM_dynamic(x_embedd=embedded_chars, input_seq_len=input_seq_len, cell_clip=None, shared_fw_bw=True)\n",
    "output_fw, output_bw = outputs\n",
    "states_fw, states_bw = states\n",
    "\n",
    "# we use concatenation over the fw and bw outputs - some approaches use sum?\n",
    "# output_concat = tf.concat(2, outputs) - we have to reverse and concat\n",
    "\n",
    "output_bw_reversed = tf.reverse(output_bw,dims=(False, False, True))\n",
    "outputs_rev = [output_fw, output_bw_reversed]\n",
    "\n",
    "output_concat = tf.concat(2, outputs_rev)\n",
    "\n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "bi_output_concat_flat = tf.reshape(output_concat, [-1, 2*hidden_size])\n",
    "# bi_output_concat_flat_clipped = tf.clip_by_value(bi_output_concat_flat, -1., 1.)\n",
    "\n",
    "logits_flat = tf.batch_matmul(bi_output_concat_flat, weights[\"out\"]) + biases[\"out\"]\n",
    "# logits_flat = tf.clip_by_value(logits_flat, -1., 1.)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    "\n",
    "print \"outputs[-1].shape:%s\" % outputs[-1]#\n",
    "print \"weights[\\\"out\\\"]].shape:%s\" % weights[\"out\"]#.shape\n",
    "print \"biases[\\\"out\\\"].shape:%s\" % biases[\"out\"]#.shape\n",
    "\n",
    "# print pred\n",
    "\n",
    "#make y flat so it match pred shape\n",
    "input_y_flat =  tf.reshape(input_y, [-1])\n",
    "print \"input_y_flat:%s\" % input_y_flat\n",
    "# # Define loss and optimizer\n",
    "\n",
    "def tf_nan_to_zeros_float64(tensor):\n",
    "    return tf.select(tf.is_nan(tensor), tf.zeros(tf.shape(tensor), dtype=tf.float64),tensor)\n",
    "# sparse softmax cros entropy - aka - calculates on non-onehot y!\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_flat, input_y_flat) \n",
    "# replace nans with zeros\n",
    "losses = tf_nan_to_zeros_float64(losses)\n",
    "\n",
    "learn_rate = tf.Variable(learning_rate, trainable=False)\n",
    "\n",
    "#Applying compute and apply gradients\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(losses)\n",
    "capped_gvs = [(tf.clip_by_value(tf_nan_to_zeros_float64(grad), -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "apply_grads_op=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "## Calculate the accuracy\n",
    "# Mask the losses - padded values are zeros\n",
    "mask = tf.sign(tf.cast(input_y_flat, dtype=tf.float64))\n",
    "print \"mask:%s\" % mask\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Bring back to [batch, class_num] shape\n",
    "masked_losses = tf.reshape(masked_losses,  tf.shape(input_y))\n",
    "\n",
    "input_seq_len_float = tf.cast(input_seq_len, dtype=tf.float64)\n",
    "\n",
    "# Calculate mean loss - depending on the dynamic number of elements\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=1) / input_seq_len_float\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "# # Evaluate model\n",
    "print \"probs_flat:%s\" % probs_flat\n",
    "print \"input_y:%s\" % input_y\n",
    "\n",
    "preds_flat = tf.argmax(probs_flat,1)\n",
    "\n",
    "preds_non_paddings = tf.gather(preds_flat, tf.where(tf.greater(input_y_flat, [0])))\n",
    "input_y_non_paddings = tf.gather(input_y_flat, tf.where(tf.greater(input_y_flat, [0])))\n",
    "\n",
    "preds_y = tf.reshape(preds_flat, tf.shape(input_y))\n",
    "\n",
    "correct_pred = tf.equal(preds_non_paddings, input_y_non_paddings)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n",
    "\n",
    "print \"The model might be okay :D\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "embeddings_random = 0.01*np.random.uniform(size=(embeddings_number, embedding_size))\n",
    "# embeddings_random[0][0] = float('nan')\n",
    "\n",
    "nan_vals = 0\n",
    "\n",
    "print np.isnan(np.sum(embeddings_random))\n",
    "# for i in range(len(embeddings_random)):\n",
    "#     for j in range(embeddings_random.shape[1]):\n",
    "#         if np.isNan(embeddings_random[i][j]):\n",
    "#             nan_vals +=1\n",
    "# print nan_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0, 0, 0, 0, 0]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "def pad(seq, pad_value, to_size):\n",
    "    pad_seq = []\n",
    "    if len(seq) > to_size:\n",
    "        pad_seq = seq[:to_size]\n",
    "    else:\n",
    "        pad_seq = seq[:] + [pad_value]*(to_size - len(seq))\n",
    "        \n",
    "    return pad_seq\n",
    "\n",
    "print pad([1,2,3], 0, 8)\n",
    "print pad([1,2,3,4,5,6,7,8], 0, 8)\n",
    "print pad([1,2,3,4,5,6,7,8,9,10], 0, 8)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'O', 2: u'B-EVENT', 3: u'I-EVENT'}\n",
      "None       (0) accuracy = 1.0\n",
      "O          (1) accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix([0, 0, 1, 1], [0,0,1,0])\n",
    "classes_dict = dict([(v,k) for (k,v) in train_seq.y_dict.items()])\n",
    "print classes_dict\n",
    "\n",
    "def print_acc_from_conf_matrix(conf_matrix, classes_dict):\n",
    "    for lblid in range(conf_matrix.shape[0]):\n",
    "        overl_cnt = np.sum(conf_matrix[lblid])\n",
    "        true_cnt = conf_matrix[lblid][lblid]\n",
    "        lbl_acc = float(true_cnt)/float(overl_cnt) if overl_cnt > 0 else 1\n",
    "        lbl_name = classes_dict[lblid] if lblid in classes_dict else \"None\"\n",
    "        print \"%s (%s) accuracy = %s\"%(lbl_name.ljust(10), lblid, lbl_acc)\n",
    "\n",
    "print_acc_from_conf_matrix(conf_matrix, classes_dict)\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7fd14bf514d0>> ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches:251\n",
      "Start training in 10 epochs\n",
      "Epoch 1:  train epoch time 315.011768103 \n",
      "Confusion matrix:\n",
      "[[     0      0      0      0]\n",
      " [    42 164509    992      0]\n",
      " [     3   4267   2019      0]\n",
      " [     0    313     33      0]]\n",
      "Accuracy by class:\n",
      "None       (0) accuracy = 1\n",
      "O          (1) accuracy = 0.993753888718\n",
      "B-EVENT    (2) accuracy = 0.3210367308\n",
      "I-EVENT    (3) accuracy = 0.0\n",
      " Dev cost 0.08 | Dev acc 0.98 % in 205.470056057\n",
      "\n",
      "Epoch 2:  train epoch time 423.340708971 \n",
      "Confusion matrix:\n",
      "[[     0      0      0      0]\n",
      " [     4 164005   1534      0]\n",
      " [     2   2752   3535      0]\n",
      " [     0    271     75      0]]\n",
      "Accuracy by class:\n",
      "None       (0) accuracy = 1\n",
      "O          (1) accuracy = 0.990709362522\n",
      "B-EVENT    (2) accuracy = 0.562092542535\n",
      "I-EVENT    (3) accuracy = 0.0\n",
      " Dev cost 0.08 | Dev acc 0.98 % in 203.339894056\n",
      "\n",
      "Epoch 3:  train epoch time 315.096184015 \n",
      "Confusion matrix:\n",
      "[[     0      0      0      0]\n",
      " [     9 164487   1047      0]\n",
      " [     6   3067   3216      0]\n",
      " [     0    280     66      0]]\n",
      "Accuracy by class:\n",
      "None       (0) accuracy = 1\n",
      "O          (1) accuracy = 0.993620992733\n",
      "B-EVENT    (2) accuracy = 0.511369057084\n",
      "I-EVENT    (3) accuracy = 0.0\n",
      " Dev cost 0.09 | Dev acc 0.98 % in 235.657105923\n",
      "\n",
      "Epoch 4:  train epoch time 332.377794981 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a9afb77296f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mbatch_data_padded_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data_padded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data_seqlens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mres_pred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data_padded_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data_padded_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_data_seqlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;31m#         print batch_data_padded_y[10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a9afb77296f5>\u001b[0m in \u001b[0;36mdev_step\u001b[0;34m(x_batch, y_batch, x_batch_seq_len)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 ], \n\u001b[0;32m--> 167\u001b[0;31m                feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# print \" Dev cost %2.2f | Dev batch acc %2.2f %% in %s\\n\" % (res_cost, res_acc, ti.time()-start),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time as ti\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init, feed_dict = None)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_batch_data(data, pad_value=0):\n",
    "    batch_data_seqlens = [len(a.x) for a in data]\n",
    "    max_len = max(batch_data_seqlens)\n",
    "\n",
    "    batch_data_padded_x = [pad(a.x.tolist(), pad_value, max_len) for a in data]\n",
    "    batch_data_padded_y = [pad(a.y.tolist(), pad_value, max_len) for a in data]\n",
    "    # batch_data_padded_x_embedd = [[pad(a.x.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "    # batch_data_padded_y_mask = [[1 if item>0 else 0 for item in pad(a.y.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "\n",
    "    return batch_data_padded_x, batch_data_padded_y, batch_data_seqlens\n",
    "\n",
    "def train_step(x_batch, y_batch, x_batch_seq_len):\n",
    "\n",
    "    feed_dict = {\n",
    "        embeddings_placeholder : embeddings_random,\n",
    "        input_x : x_batch, # batch_data_padded_x,\n",
    "        input_y : y_batch, # batch_data_padded_y,\n",
    "        input_seq_len: x_batch_seq_len # batch_data_seqlens\n",
    "    }\n",
    "\n",
    "    #_, \\\n",
    "    _,\\\n",
    "    res_cost, \\\n",
    "    res_acc, \\\n",
    "    res_output_y,\\\n",
    "    res_mean_loss_by_example, \\\n",
    "    res_logits_flat,\\\n",
    "    res_losses, \\\n",
    "    res_probs_flat, \\\n",
    "    res_input_seq_len_float, \\\n",
    "    res_input_y_flat, res_bi_output_concat_flat, \\\n",
    "    res_output_fw, res_output_bw_reversed, res_embedded_chars, \\\n",
    "    res_weights_out, res_biases_out, \\\n",
    "    res_capped_gvs, \\\n",
    "    res_gvs \\\n",
    "    = sess.run([\n",
    "                apply_grads_op,\n",
    "                mean_loss, \n",
    "                accuracy, \n",
    "                preds_y, \n",
    "                mean_loss_by_example, \n",
    "                logits_flat,\n",
    "                losses, \n",
    "                probs_flat,\n",
    "                input_seq_len_float,\n",
    "                input_y_flat, bi_output_concat_flat,\n",
    "                output_fw, output_bw_reversed, embedded_chars,\n",
    "                weights[\"out\"], biases[\"out\"],\n",
    "                capped_gvs,\n",
    "                gvs], \n",
    "               feed_dict=feed_dict)\n",
    "\n",
    "    check_for_nans = True\n",
    "\n",
    "    \n",
    "    non_zero_input = [oy for oy in y_batch if 2 in oy]\n",
    "    # print \"Non_zero_input:%s\" % len(non_zero_input)\n",
    "    non_zero_preds = [oy for oy in res_output_y if 2 in oy]\n",
    "    # print \"Non-zero preds:%s\" % len(non_zero_preds)\n",
    "    \n",
    "    # print non_zero_preds[:3]\n",
    "    if check_for_nans:\n",
    "        # print \"res_input_seq_len_float:%s\" % res_input_seq_len_float\n",
    "        has_nans = False\n",
    "        # res_output_concat =  sess.run(output_concat, feed_dict=feed_dict)\n",
    "        # print res_output_concat.shape\n",
    "        if np.isnan(np.sum(res_embedded_chars)):\n",
    "            has_nans = True\n",
    "            print \"[res_embedded_chars] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_output_fw)):\n",
    "            has_nans = True\n",
    "            print \"[res_output_fw] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_output_bw_reversed)):\n",
    "            has_nans = True\n",
    "            print \"[res_output_bw_reversed] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_bi_output_concat_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_bi_output_concat_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_weights_out)):\n",
    "            has_nans = True\n",
    "            print \"[res_weights_out] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_biases_out)):\n",
    "            has_nans = True\n",
    "            print \"[res_biases_out] has nans!\"            \n",
    "\n",
    "        if np.isnan(np.sum(res_logits_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_logits_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_probs_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_probs_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_losses)):\n",
    "            has_nans = True\n",
    "            print \"[res_losses] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_mean_loss_by_example)):\n",
    "            has_nans = True\n",
    "            print \"[res_mean_loss_by_example] has nans!\"\n",
    "\n",
    "        if np.isnan(res_cost):\n",
    "            has_nans = True\n",
    "            print \"[res_cost] has nans!\"\n",
    "\n",
    "#         if np.isnan(res_gvs):\n",
    "#             has_nans = True\n",
    "#             print \"[res_gvs] has nans!\"\n",
    "\n",
    "#         if np.isnan(res_capped_gvs):\n",
    "#             has_nans = True\n",
    "#             print \"[res_capped_gvs] has nans!\"\n",
    "\n",
    "        if has_nans:\n",
    "            print \"res_weights_out:%s\" % res_weights_out[:5]\n",
    "            print \"res_biases_out:%s\" % res_biases_out\n",
    "            print \"batch_data_padded_x:%s\" % batch_data_padded_x[0]\n",
    "            print \"res_embedded_chars:%s\" % len(res_embedded_chars), len(res_embedded_chars[0])\n",
    "            print \"res_output_fw:%s\" % res_output_fw[0]\n",
    "            print \"res_output_bw_reversed:%s\" % res_output_bw_reversed[0]\n",
    "            # print \"res_bi_output_concat_flat:%s\" % res_bi_output_concat_flat\n",
    "            print \"res_input_y_flat:%s\" % res_input_y_flat\n",
    "            print \"res_logits_flat:%s\" % res_logits_flat\n",
    "            print \"res_res_probs_flat:%s\" % res_probs_flat\n",
    "            print \"res_mean_loss_by_example:%s\" % res_mean_loss_by_example\n",
    "            print \"res_losses:%s\" % res_losses\n",
    "            print \"res_capped_gvs:%s\" % res_capped_gvs\n",
    "            print \"res_gvs:%s\" % res_gvs\n",
    "\n",
    "            # print losses by example\n",
    "            res_losses_reshaped = res_losses.reshape([50, res_losses.shape[0]/50])\n",
    "            for i in range(len(res_losses_reshaped)):\n",
    "                if np.isnan(np.sum(res_losses_reshaped[i])):\n",
    "                    print res_losses_reshaped[i]\n",
    "            raise Exception(\"Fucking NANS!\")\n",
    "\n",
    "    return res_cost, res_acc\n",
    "        \n",
    "def dev_step(x_batch, y_batch, x_batch_seq_len):\n",
    "    \n",
    "    feed_dict = {\n",
    "        embeddings_placeholder : embeddings_random,\n",
    "        input_x : x_batch, # batch_data_padded_x,\n",
    "        input_y : y_batch, # batch_data_padded_y,\n",
    "        input_seq_len: x_batch_seq_len # batch_data_seqlens\n",
    "    }\n",
    "\n",
    "    # _ ,\\\n",
    "    res_cost, \\\n",
    "    res_acc, \\\n",
    "    res_output_y,\\\n",
    "    res_mean_loss_by_example, \\\n",
    "    res_logits_flat,\\\n",
    "    res_losses, \\\n",
    "    res_probs_flat, \\\n",
    "    res_input_seq_len_float, \\\n",
    "    res_input_y_flat, res_bi_output_concat_flat, \\\n",
    "    res_output_fw, res_output_bw_reversed, res_embedded_chars, \\\n",
    "    res_weights_out, res_biases_out, \\\n",
    "    res_capped_gvs, \\\n",
    "    res_gvs \\\n",
    "    = sess.run([\n",
    "                # apply_grads_op, - commented so does not apply gradients\n",
    "                mean_loss, \n",
    "                accuracy, \n",
    "                preds_y,\n",
    "                mean_loss_by_example, \n",
    "                logits_flat,\n",
    "                losses, \n",
    "                probs_flat,\n",
    "                input_seq_len_float,\n",
    "                input_y_flat, bi_output_concat_flat,\n",
    "                output_fw, output_bw_reversed, embedded_chars,\n",
    "                weights[\"out\"], biases[\"out\"],\n",
    "                capped_gvs,\n",
    "                gvs\n",
    "                ], \n",
    "               feed_dict=feed_dict)\n",
    "\n",
    "    # print \" Dev cost %2.2f | Dev batch acc %2.2f %% in %s\\n\" % (res_cost, res_acc, ti.time()-start),\n",
    "\n",
    "    return res_output_y, res_cost, res_acc\n",
    "\n",
    "pad_value = 0\n",
    "train_batches_cnt = len(train_seq)/batch_size\n",
    "dev_batches_cnt = len(dev_seq)/batch_size\n",
    "\n",
    "print \"Batches:%s\" % train_batches_cnt\n",
    "#batches_cnt = min(batches_cnt, 1)\n",
    "epochs_cnt = 10\n",
    "#train with batches\n",
    "max_len = 0\n",
    "print \"Start training in %s epochs\" % epochs_cnt\n",
    "for epoch in range(0, epochs_cnt):\n",
    "    print \"Epoch %d:\" % (epoch+1),\n",
    "    start_epoch = ti.time()\n",
    "    \n",
    "    for i in range(0, train_batches_cnt):\n",
    "        # print\"Batch %s =================\" % (i+1)\n",
    "        \n",
    "        #Get the batch\n",
    "        batch_data = train_seq[i*batch_size:i*batch_size+batch_size]\n",
    "        batch_data_padded_x, batch_data_padded_y, batch_data_seqlens = prepare_batch_data(data=batch_data)\n",
    "\n",
    "        #Do the train step\n",
    "        start = ti.time()\n",
    "        cost, acc = train_step(x_batch = batch_data_padded_x, y_batch = batch_data_padded_y, x_batch_seq_len = batch_data_seqlens)\n",
    "        # print \" Train cost %2.2f | Train batch acc %2.2f %% in %s\\n\" % (cost, acc, ti.time()-start)\n",
    "        \n",
    "    print \" train epoch time %s \" % (ti.time()-start_epoch)\n",
    "    #Dev eval - once per epoch\n",
    "    input_y_all = []\n",
    "    pred_y_all = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, dev_batches_cnt):\n",
    "        batch_data = dev_seq[i*batch_size:i*batch_size+batch_size]\n",
    "        batch_data_padded_x, batch_data_padded_y, batch_data_seqlens = prepare_batch_data(data=batch_data)\n",
    "    \n",
    "        res_pred_y, cost, acc = dev_step(x_batch = batch_data_padded_x, y_batch = batch_data_padded_y, x_batch_seq_len = batch_data_seqlens)\n",
    "        \n",
    "#         print batch_data_padded_y[10]\n",
    "#         print res_pred_y[10]\n",
    "        \n",
    "        for j in range(0, batch_size):\n",
    "            input_y_all.extend(batch_data_padded_y[j][:batch_data_seqlens[j]])\n",
    "            pred_y_all.extend(res_pred_y[j][:batch_data_seqlens[j]])\n",
    "            \n",
    "            \n",
    "    # eval\n",
    "    print \"Confusion matrix:\"\n",
    "    conf_matrix = confusion_matrix(input_y_all, pred_y_all)\n",
    "    print conf_matrix\n",
    "    print \"Accuracy by class:\"\n",
    "    print_acc_from_conf_matrix(conf_matrix, classes_dict)\n",
    "    \n",
    "#     print \"Class  : P, R, F-Score:\" \n",
    "#     print \"B-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=2)\n",
    "#     print \"I-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=3)\n",
    "#     \n",
    "    print \" Dev cost %2.2f | Dev acc %2.2f %% in %s\\n\" % (cost, acc, ti.time()-start)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import time as ti\n",
    "# pad_value = 0\n",
    "# batches_cnt = len(train_seq)/batch_size\n",
    "# print \"Batches:%s\" % batches_cnt\n",
    "# #batches_cnt = min(batches_cnt, 1)\n",
    "\n",
    "# # Initializing the variables\n",
    "# init = tf.initialize_all_variables()\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(init, feed_dict = {embeddings_placeholder : embeddings_random})\n",
    "\n",
    "\n",
    "# epochs_cnt = 10\n",
    "# #train with batches\n",
    "# print \"Start training in %s epochs\" % epochs_cnt\n",
    "# for epoch in range(0, epochs_cnt):\n",
    "#     print \"Epoch %d:\" % (epoch+1),\n",
    "#     start = ti.time()\n",
    "#     for i in range(0, batches_cnt):\n",
    "#         print\"batch %s =================\" %(i+1)\n",
    "#         #print i\n",
    "#         batch_data = train_seq[i*batch_size:i*batch_size+batch_size]\n",
    "\n",
    "#         batch_data_seqlens = [len(a.x) for a in batch_data]\n",
    "#         #print data_lens\n",
    "#         max_len = max(batch_data_seqlens)\n",
    "#         #print max_len\n",
    "#         batch_data_padded_x = [pad(a.x.tolist(), pad_value, max_len) for a in batch_data]\n",
    "#         batch_data_padded_x_embedd = [[pad(a.x.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "#         batch_data_padded_y = [pad(a.y.tolist(), pad_value, max_len) for a in batch_data]\n",
    "#         batch_data_padded_y_mask = [[1 if item>0 else 0 for item in pad(a.y.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "\n",
    "#         feed_dict = {\n",
    "#             embeddings_placeholder : embeddings_random,\n",
    "#             input_x : batch_data_padded_x,\n",
    "#             input_y : batch_data_padded_y,\n",
    "#             input_y_mask : batch_data_padded_y_mask,\n",
    "#             input_seq_len: batch_data_seqlens\n",
    "#         }\n",
    "\n",
    "#         _, res_cost, res_acc, res_mean_loss_by_example, \\\n",
    "#                 res_losses, res_input_y_flat, res_bi_output_concat_flat,\\\n",
    "#             res_output_fw, res_output_bw_reversed, res_embedded_chars = \\\n",
    "#                  sess.run([optimizer, mean_loss, accuracy, mean_loss_by_example, \n",
    "#                            losses, input_y_flat, bi_output_concat_flat,\n",
    "#                           output_fw, output_bw_reversed, embedded_chars], \n",
    "#                  feed_dict=feed_dict)\n",
    "#         print \"batch %s cost=%s, acc=%s:\" %(i+1, res_cost, res_acc)\n",
    "#         # res_output_concat =  sess.run(output_concat, feed_dict=feed_dict)\n",
    "#         # print res_output_concat.shape\n",
    "#         print \"batch_data_padded_x:%s\" % batch_data_padded_x[0]\n",
    "#         has_nans =  np.isnan(np.sum(res_embedded_chars))\n",
    "#         print \"res_embedded_chars:%s\" % len(res_embedded_chars), len(res_embedded_chars[0])\n",
    "#         print \"res_output_fw:%s\" % res_output_fw[0]\n",
    "#         print \"res_output_bw_reversed:%s\" % res_output_bw_reversed[0]\n",
    "#         print \"res_bi_output_concat_flat:%s\" % res_bi_output_concat_flat\n",
    "#         print \"res_input_y_flat:%s\" % res_input_y_flat\n",
    "#         print \"res_mean_loss_by_example:%s\" % res_mean_loss_by_example\n",
    "#         print \"res_losses:%s\" % res_losses\n",
    "#         if has_nans:\n",
    "#             raise Exception(\"Fucking NANS!\")\n",
    "    \n",
    "#     print \" Train cost %2.2f Train Acc %2.2f %% in %s\\n\" % (res_cost, res_acc, ti.time()-start),\n",
    "#     #print batch_data_padded_x\n",
    "#     #print batch_data_padded_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
