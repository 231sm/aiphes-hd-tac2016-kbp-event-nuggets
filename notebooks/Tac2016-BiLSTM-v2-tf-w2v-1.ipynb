{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1}\n",
      "{0: 'a', 1: 'b'}\n"
     ]
    }
   ],
   "source": [
    "# LSTM v1 - Simple Lstm experiments\n",
    "from sequences.label_dictionary import LabelDictionary\n",
    "dic = LabelDictionary([], start_index=0)\n",
    "\n",
    "dic.add(\"a\")\n",
    "dic.add(\"b\")\n",
    "print dic\n",
    "\n",
    "print dic.names\n",
    "\n",
    "assert(\"a\" == dic.get_label_name(dic[\"a\"]))\n",
    "assert(\"b\" == dic.get_label_name(dic[\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data/\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import readers.tac_prep_corpus as tc\n",
    "from readers.tac_prep_corpus import *\n",
    "\n",
    "from sequences.sequence_list import SequenceList\n",
    "\n",
    "print data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tac2014_train = data_dir+\"/clear_data/data_tac2014_train.json\"\n",
    "data_tac2014_eval = data_dir+\"/clear_data/data_tac2014_eval.json\"\n",
    "data_tac2015_train = data_dir+\"/clear_data/data_tac2015_train.json\"\n",
    "data_tac2015_eval = data_dir+\"/clear_data/data_tac2015_eval.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'.', 18754), (u'the', 16829), (u',', 15016), (u'to', 9928), (u'a', 8518), (u'of', 8155), (u'and', 7597), (u'in', 6388), (u'I', 4800), (u'that', 4490)]\n"
     ]
    }
   ],
   "source": [
    "# reload(readers.tac_prep_corpus)\n",
    "word_counts = TacPrepJsonCorpus.word_counts_from_jsonfiles(json_files = [data_tac2014_train, data_tac2014_eval, data_tac2015_train])\n",
    "\n",
    "# print \"Top 10 tokens:\"\n",
    "print word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:20459\n",
      "Tokens with freq>=2 : 15097\n"
     ]
    }
   ],
   "source": [
    "# word_counts.sort(key = lambda x:x[1], reverse=True)\n",
    "print \"Vocab size:%s\" % len(word_counts)\n",
    "print \"Tokens with freq>=%s : %s\" % (2, len( [xx for xx in word_counts if xx[1]>=2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNKNWN>', u'.', u'the', u',', u'to', u'a', u'of', u'and', u'in']\n"
     ]
    }
   ],
   "source": [
    "unknown_word = \"<UNKNWN>\"\n",
    "pad_word = \"<PAD>\"\n",
    "# clear the vocab\n",
    "vocab = [xx[0] for xx in word_counts]\n",
    "vocab.insert(0, unknown_word)\n",
    "vocab.insert(0, pad_word)\n",
    "print vocab[:10]\n",
    "\n",
    "vocab_dict = LabelDictionary(vocab, start_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "print len([xx for xx in vocab if (\"/\" in xx)or(\"\\\\\" in xx)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104633\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model_filename = data_dir+\"../resources/external/w2v_embeddings/\" + \"qatarliving_qc_size20_win10_mincnt5_rpl_skip1_phrFalse_2016_02_23.word2vec.bin\"\n",
    "word2vec_model_binary = False\n",
    "\n",
    "# word2vec_model = Word2Vec.load_word2vec_format(word2vec_model_filename, binary=word2vec_model_binary)\n",
    "word2vec_model = Word2Vec.load(word2vec_model_filename)\n",
    "\n",
    "index2word = word2vec_model.index2word\n",
    "index2word_set = set(index2word)\n",
    "print len(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'distributers', u'mogalina', u'gag', u'woods', u'spiders', u'woody', u'trawling', u'regularize', u'canes', u'erfectly']\n"
     ]
    }
   ],
   "source": [
    "print index2word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in w2v freq>=2 : 18906\n"
     ]
    }
   ],
   "source": [
    "# words in training data which are in the w2v embeddings dictionary\n",
    "\n",
    "print \"Tokens in w2v freq>=%s : %s\" % (2, len( [xx for xx in word_counts if xx[1]>1 or xx[0] in index2word_set or xx[0].lower() in index2word_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 16829), (u'to', 9928), (u'a', 8518), (u'of', 8155), (u'and', 7597), (u'in', 6388), (u'that', 4490), (u'is', 4143), (u'for', 3738), (u'was', 3434)]\n"
     ]
    }
   ],
   "source": [
    "# merge the index2word with the dictionary of the corpus\n",
    "\n",
    "word_counts_dict = dict(word_counts)\n",
    "\n",
    "full_dict_word_counts = {}\n",
    "for wi in range(0, len(index2word)):\n",
    "    if index2word[wi] in word_counts_dict:\n",
    "        full_dict_word_counts[index2word[wi]] = word_counts_dict[index2word[wi]]\n",
    "    else:\n",
    "        full_dict_word_counts[index2word[wi]] = 0\n",
    "        \n",
    "full_dict_word_counts = [(k,v) for (k,v) in full_dict_word_counts.items()]\n",
    "full_dict_word_counts.sort(key=lambda f: f[1], reverse=True)\n",
    "\n",
    "print full_dict_word_counts[:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76778793335\n",
      "unknown_vec:[ 0.07160316  0.06742626  0.15077273 -0.07981906 -0.11572608 -0.09256729\n",
      " -0.08221466 -0.09356559  0.01780725 -0.0515666   0.05499313 -0.10132896\n",
      " -0.09117364  0.16332768 -0.0177103  -0.07968703  0.07441449  0.10293729\n",
      " -0.13497648 -0.10935152]\n"
     ]
    }
   ],
   "source": [
    "# from Word2Vec_AverageVectorsUtilities import AverageVectorUtilities\n",
    "import numpy as np\n",
    "import time as ti\n",
    "def get_average_vector(words, model, num_features, index2word_set):\n",
    "    #function to average all words vectors in a given paragraph\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    #list containing names of words in the vocabulary\n",
    "    #index2word_set = set(word2vec_model.index2word) this is moved as input param for performance reasons\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "index2word_set=set(index2word)\n",
    "st = ti.time()\n",
    "unknown_vec = get_average_vector(index2word, word2vec_model, word2vec_model.syn0.shape[1], index2word_set)\n",
    "print ti.time()-st\n",
    "\n",
    "print \"unknown_vec:%s\" % unknown_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embeddings...\n",
      "20461\n",
      "Done in 0.323060035706 s\n"
     ]
    }
   ],
   "source": [
    "from VocabEmbedding_Utilities import VocabEmbeddingUtilities\n",
    "\n",
    "st = ti.time()\n",
    "\n",
    "vocab_and_embeddings = VocabEmbeddingUtilities.get_embeddings_for_vocab_from_model(vocabulary=vocab_dict, \n",
    "                                                            embeddings_type='w2v', \n",
    "                                                            embeddings_model=word2vec_model, \n",
    "                                                            embeddings_size=word2vec_model.syn0.shape[1])\n",
    "\n",
    "#set the unknown embeddings to the average of all word embeddings\n",
    "vocab_and_embeddings[\"embeddings\"][vocab_and_embeddings[\"vocabulary\"][unknown_word],:] = unknown_vec\n",
    "\n",
    "# print unknown_vec\n",
    "# print vocab_and_embeddings[\"embeddings\"][vocab_and_embeddings[\"vocabulary\"][unknown_word]]\n",
    "print \"Done in %s s\" %(ti.time()-st)\n",
    "# print vocab_and_embeddings[\"vocabulary\"]\n",
    "# print vocab_and_embeddings[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sent_len = 1000 \n",
    "max_nr_sent = 100000\n",
    "max_sent_len=max_sent_len\n",
    "max_nr_sent=max_nr_sent\n",
    "update_vocab=False\n",
    "update_tags=False\n",
    "\n",
    "unknown_tag = u'O'\n",
    "mapping_file = None\n",
    "data_x_fieldname = \"tokens\"\n",
    "data_y_fieldname = \"labels_event\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 17.8586909771 s..\n",
      "[(u'O', 402099), (u'B-EVENT', 15656), (u'I-EVENT', 553)]\n"
     ]
    }
   ],
   "source": [
    "st = ti.time()\n",
    "labels_counts = TacPrepJsonCorpus.word_counts_from_jsonfiles(json_files = [data_tac2014_train, data_tac2014_eval, data_tac2015_train], data_fieldname=data_y_fieldname)\n",
    "print \"Done in %s s..\" %(ti.time()-st)\n",
    "print labels_counts\n",
    "\n",
    "labels_sorted_alpha = sorted(labels_counts, key=lambda xx:xx[0])\n",
    "labels_lst = [unknown_tag] + [xx[0] for xx in labels_sorted_alpha if xx[0] <> unknown_tag]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'O', u'B-EVENT', u'I-EVENT']\n"
     ]
    }
   ],
   "source": [
    "print labels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_non_zero_label(test_seq, zero_label=0):\n",
    "    cnt = 0\n",
    "    for item in test_seq:\n",
    "        for lbl in item.y:\n",
    "            if lbl != zero_label:\n",
    "                cnt += 1\n",
    "                # print item.y\n",
    "                break\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use indexes that start from 1 if you use TF dynamic sequences!\n",
    "tag_start_index = 1\n",
    "vocab_start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "corpus_vocab_input = LabelDictionary()\n",
    "corpus_vocab_input.set_dict(vocab_and_embeddings[\"vocabulary\"])\n",
    "\n",
    "tac_corpus = TacPrepJsonCorpus([], labels_lst, \n",
    "                               tag_start_index=tag_start_index, \n",
    "                               vocab_start_index=vocab_start_index)\n",
    "\n",
    "tac_corpus.set_word_dict(corpus_vocab_input)\n",
    "\n",
    "print tac_corpus.word_dict[unknown_word]\n",
    "print tac_corpus.sequence_list.x_dict[unknown_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2015_eval.json\n"
     ]
    }
   ],
   "source": [
    "test_seq, test_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2015_eval], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print tac_corpus.word_dict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:4164\n",
      "With non zero labels:2646\n",
      "[u'regime', u'<img\\xa0src=\"http://www.amnesty.org/sites/impact.amnesty.org/files/imagecache/news-highlight/brazil-amnesty-law%2025.04.12.JPG\"/>', u'The', u'efforts', u'of', u'federal', u'prosecutors', u'to', u'initiate', u'criminal', u'investigations', u'into', u'past', u'human', u'rights', u'violations', u'marks', u'a', u'crucial', u'moment', u'in', u'Brazil', u\"'s\", u'history', u',', u'said', u'Amnesty', u'International', u'after', u'federal', u'prosecutors', u'in', u'S\\xe3o', u'Paulo', u'charged', u',', u'on', u'24', u'March', u'2012', u',', u'retired', u'Colonel', u'Carlos', u'Alberto', u'Brilhante', u'Ustra', u'and', u'police', u'chief', u'-LRB-', u'delegado', u'-RRB-', u'Dirceu', u'Garvina', u',', u'with', u'the', u'kidnapping', u'of', u'union', u'leader', u'Alu\\xedzio', u'Palhano', u'Pedreira', u'Ferreira', u'in', u'1971', u'.']\n",
      "[1647, 1, 27, 1228, 7, 392, 449, 5, 1, 430, 3638, 121, 406, 563, 260, 2657, 1, 6, 3300, 1927, 9, 776, 15, 584, 4, 36, 15620, 1871, 85, 392, 449, 9, 1, 8838, 295, 4, 18, 1762, 955, 1456, 4, 1395, 6967, 2690, 4879, 1, 1, 8, 147, 695, 49, 1, 43, 1, 1, 4, 23, 3, 3273, 7, 1885, 459, 1, 1, 1, 1, 9, 5530, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "data fields:\n",
      "tokens\n",
      "deps_basic\n",
      "lemmas\n",
      "deps_cc\n",
      "pos\n",
      "parse\n",
      "file_name\n",
      "file_id\n",
      "char_offsets\n",
      "labels_realis\n",
      "labels_type_full\n",
      "labels_event\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"All sents:%s\" % len(test_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(test_seq, zero_label = tag_start_index)\n",
    "print test_seq_meta[0][\"tokens\"]\n",
    "print test_seq[0].x\n",
    "print test_seq[0].y\n",
    "\n",
    "print \"data fields:\"\n",
    "for k,v in  test_seq_meta[0].iteritems():\n",
    "    print k\n",
    "# print [tac_corpus.word_dict[a] for a in test_seq[0].x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2014_train.json\n",
      "readers/../data//clear_data/data_tac2015_train.json\n"
     ]
    }
   ],
   "source": [
    "train_seq, train_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2014_train, data_tac2015_train], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:12573\n",
      "With non zero labels:5367\n"
     ]
    }
   ],
   "source": [
    "print \"All sents:%s\" % len(train_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(train_seq, zero_label = tag_start_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readers/../data//clear_data/data_tac2014_eval.json\n"
     ]
    }
   ],
   "source": [
    "dev_seq, dev_seq_meta = tac_corpus.read_sequence_list_tac_json([data_tac2014_eval], max_sent_len=max_sent_len, max_nr_sent=max_nr_sent, \n",
    "                         update_vocab=update_vocab, update_tags=update_tags, unknown_word=unknown_word, \n",
    "                         unknown_tag=unknown_tag, mapping_file=mapping_file, \n",
    "                         data_x_fieldname=data_x_fieldname,\n",
    "                         data_y_fieldname=data_y_fieldname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sents:9041\n",
      "With non zero labels:3749\n"
     ]
    }
   ],
   "source": [
    "print \"All sents:%s\" % len(dev_seq)\n",
    "print \"With non zero labels:%s\" % count_non_zero_label(dev_seq, zero_label = tag_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'B-EVENT': 2, u'O': 1, u'I-EVENT': 3}\n"
     ]
    }
   ],
   "source": [
    "print train_seq.y_dict\n",
    "\n",
    "# Redo indices\n",
    "# train_seq, test_seq, dev_seq = tc.compacify(train_seq, test_seq, dev_seq,\n",
    "#                                             default_vocab_dict=[unknown_word], \n",
    "#                                             vocab_start_index=vocab_start_index, \n",
    "#                                             default_tag_dict=[unknown_tag], \n",
    "#                                             tag_start_index=tag_start_index,\n",
    "#                                             theano=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get number of words and tags in the corpus\n",
    "nr_words = len(train_seq.x_dict)\n",
    "nr_tags = len(train_seq.y_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20461\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for k,v in train_seq.x_dict.iteritems():\n",
    "    #print k,v\n",
    "    if v>nr_words:\n",
    "        print (k,v)\n",
    "        \n",
    "print nr_words\n",
    "print nr_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lstm_v1 as lstm_v1\n",
    "# import theano\n",
    "# import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr_words:20461\n",
      "emb_size:50\n",
      "hidden_size:50\n",
      "nr_tags:3\n",
      "lrate:0.0001\n"
     ]
    }
   ],
   "source": [
    "emb_size=50\n",
    "hidden_size=50\n",
    "SEED = 42\n",
    "\n",
    "lrate = 0.0001\n",
    "\n",
    "nr_iterations = 5\n",
    "batch_size = 50\n",
    "print \"nr_words:%s\" % nr_words\n",
    "print \"emb_size:%s\" % emb_size\n",
    "print \"hidden_size:%s\" % hidden_size\n",
    "print \"nr_tags:%s\" % nr_tags\n",
    "print \"lrate:%s\" % lrate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow padding and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# x = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"x\")\n",
    "# y = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"y\")\n",
    "# seq_len = tf.placeholder(dtype=tf.int64, shape=[None], name = \"seq_len\")\n",
    "\n",
    "# batched_data = tf.train.batch(\n",
    "#     tensors=[x, y, seq_len],\n",
    "#     batch_size = batch_size,\n",
    "#     dynamic_pad = True,\n",
    "#     name=\"batch_data\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # data_x = np.array([np.array(a.x) for a in train_seq])\n",
    "# # data_seq_len = np.array([len(a.x) for a in train_seq])\n",
    "# # data_y = np.array([np.array(a.y) for a in train_seq])\n",
    "\n",
    "# data_x = tf.convert_to_tensor([a.x.tolist() for a in train_seq], dtype=tf.int64)\n",
    "# data_seq_len = tf.convert_to_tensor([len(a.x) for a in train_seq], dtype=tf.int64)\n",
    "# data_y = tf.convert_to_tensor([a.y.tolist() for a in train_seq], dtype=tf.int64)\n",
    "\n",
    "# print data_x[:5]\n",
    "# print data_seq_len[:5]\n",
    "# print data_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(batched_data)\n",
    "# print type(data_x)\n",
    "# print type(data_y)\n",
    "# print type(data_seq_len)\n",
    "\n",
    "# #tf.reset_default_graph()\n",
    "\n",
    "# # Run the graph\n",
    "# # tf.contrib.learn takes care of starting the queues for us\n",
    "# # res = sess.run(batched_data[0], batched_data[1], batched_data[2] , feed_dict={x:data_x, y:data_y, seq_len:data_seq_len})\n",
    "# result = tf.contrib.learn.run_n(\n",
    "#     {\n",
    "#         \"batched_data_x\": batched_data[0],\n",
    "#         \"batched_data_y\": batched_data[1],\n",
    "#         \"batched_data_seq_len\": batched_data[2]\n",
    "#     },\n",
    "#     n=1,\n",
    "#     feed_dict={x:data_x, \n",
    "#                seq_len:data_seq_len,\n",
    "#                y:data_y})\n",
    "\n",
    "\n",
    "# Print the result\n",
    "# print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\n",
    "# print(res[0][\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi LSTM\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_number:20461\n",
      "embedding_size:20\n",
      "n_classes:3\n",
      "learning_rate:0.1\n",
      "outputs[-1].shape:Tensor(\"BiLSTM/ReverseSequence:0\", shape=(?, ?, 50), dtype=float64)\n",
      "weights[\"out\"]].shape:<tensorflow.python.ops.variables.Variable object at 0x7f9c6d47a210>\n",
      "biases[\"out\"].shape:<tensorflow.python.ops.variables.Variable object at 0x7f9c6d46bc90>\n",
      "input_y_flat:Tensor(\"Reshape_1:0\", shape=(?,), dtype=int64)\n",
      "gradients:\n",
      "gradients/BatchMatMul_grad/tuple/control_dependency_1:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6d47a210>\n",
      "gradients/add_grad/tuple/control_dependency_1:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6d46bc90>\n",
      "gradients/embeddings/embedding_lookup_grad/Reshape:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6b444250>\n",
      "gradients/BiLSTM/BiRNN_FW/BiRNN_FW/while/LSTMCell/MatMul/Enter_grad/b_acc_3:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6b3b7fd0>\n",
      "gradients/BiLSTM/BiRNN_FW/BiRNN_FW/while/LSTMCell/BiasAdd/Enter_grad/b_acc_3:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6b3b7dd0>\n",
      "gradients/BiLSTM/BiRNN_BW/BiRNN_BW/while/LSTMCell/MatMul/Enter_grad/b_acc_3:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6b307cd0>\n",
      "gradients/BiLSTM/BiRNN_BW/BiRNN_BW/while/LSTMCell/BiasAdd/Enter_grad/b_acc_3:0 - <tensorflow.python.ops.variables.Variable object at 0x7f9c6b3078d0>\n",
      "mask:Tensor(\"Sign:0\", shape=(?,), dtype=float64)\n",
      "probs_flat:Tensor(\"Softmax:0\", shape=(?, 3), dtype=float64)\n",
      "input_y:Tensor(\"input_y:0\", shape=(?, ?), dtype=int64)\n",
      "The model might be okay :D\n"
     ]
    }
   ],
   "source": [
    "embeddings_number = nr_words\n",
    "embedding_size = emb_size\n",
    "embedding_size = 20 # current w2v embeddings\n",
    "n_classes = nr_tags\n",
    "learning_rate=0.1\n",
    "\n",
    "print \"embeddings_number:%s\" % embeddings_number\n",
    "print \"embedding_size:%s\" % embedding_size\n",
    "print \"n_classes:%s\" % n_classes\n",
    "print \"learning_rate:%s\" % learning_rate\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    #'out': tf.Variable(tf.random_uniform([2*hidden_size, n_classes], minval=-1, maxval=1, dtype=tf.float64), name=\"out_w\", dtype=tf.float64)\n",
    "    'out': tf.Variable(tf.truncated_normal([2*hidden_size, n_classes], stddev=0.1, dtype=tf.float64), trainable=True, name=\"out_w\", dtype=tf.float64)\n",
    "}\n",
    "biases = {\n",
    "    # 'out': tf.Variable(tf.random_uniform([n_classes], minval=-1, maxval=1, dtype=tf.float64), name=\"out_b\", dtype=tf.float64)\n",
    "    'out': tf.Variable(tf.truncated_normal([n_classes], stddev=0.1, dtype=tf.float64), trainable=True, name=\"out_b\", dtype=tf.float64)\n",
    "}\n",
    "\n",
    "input_x = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"input_x\")\n",
    "input_y = tf.placeholder(dtype=tf.int64, shape=[None, None], name = \"input_y\") # this is not onehot!\n",
    "input_seq_len = tf.placeholder(dtype=tf.int64, shape=[None], name = \"input_seq_len\")\n",
    "\n",
    "with tf.device('/cpu:0'), tf.name_scope(\"embeddings\"):\n",
    "    embeddings_placeholder = tf.placeholder(tf.float64, shape=[embeddings_number, embedding_size])\n",
    "    # embeddings_tuned = tf.Variable(embeddings_placeholder)\n",
    "    \n",
    "    # embeddings random, tuned\n",
    "    # embeddings_tuned =tf.Variable(tf.truncated_normal([embeddings_number, embedding_size], stddev=0.1, dtype=tf.float64), trainable=False, name=\"embeddings\", dtype=tf.float64)\n",
    "    \n",
    "    # embeddings, loaded, tuned\n",
    "    embeddings_tuned =tf.Variable(vocab_and_embeddings[\"embeddings\"], trainable=True, name=\"embeddings_input\", dtype=tf.float64)\n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings_tuned, input_x)\n",
    "    # embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "\n",
    "def BiLSTM_dynamic(x_embedd, input_seq_len, shared_fw_bw=False,\n",
    "                   use_peepholes=False, cell_clip=None,\n",
    "                   # initializer=None, \n",
    "                   num_proj=None, proj_clip=None,\n",
    "                   num_unit_shards=1, num_proj_shards=1,\n",
    "                   forget_bias=1.0, state_is_tuple=True,\n",
    "                   activation=tf.tanh):\n",
    "    with tf.name_scope(\"BiLSTM\"):\n",
    "        with tf.variable_scope('forward'):\n",
    "            cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True,\n",
    "                                               use_peepholes=use_peepholes, cell_clip=cell_clip,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=42),\n",
    "                                               num_proj=num_proj, proj_clip=proj_clip,\n",
    "                                               num_unit_shards=num_unit_shards, num_proj_shards=num_proj_shards,\n",
    "                                               forget_bias=forget_bias,\n",
    "                                               activation=activation)\n",
    "\n",
    "        with tf.variable_scope('backward'):    \n",
    "            cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size, state_is_tuple=True,\n",
    "                                               use_peepholes=use_peepholes, cell_clip=cell_clip,\n",
    "                                               initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=24), \n",
    "                                               num_proj=num_proj, proj_clip=proj_clip,\n",
    "                                               num_unit_shards=num_unit_shards, num_proj_shards=num_proj_shards,\n",
    "                                               forget_bias=forget_bias,\n",
    "                                               activation=activation)\n",
    "\n",
    "        outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_fw if shared_fw_bw else cell_bw,\n",
    "            dtype=tf.float64,\n",
    "            sequence_length=input_seq_len,\n",
    "            inputs=x_embedd)\n",
    "\n",
    "    return outputs, states\n",
    "\n",
    "# Dynamic BiLSTM outputs and states\n",
    "outputs, states = BiLSTM_dynamic(x_embedd=embedded_chars,\n",
    "                                 input_seq_len=input_seq_len,\n",
    "                                 cell_clip=None,\n",
    "                                 shared_fw_bw=True)\n",
    "output_fw, output_bw = outputs\n",
    "states_fw, states_bw = states\n",
    "\n",
    "\n",
    "# we use concatenation over the fw and bw outputs - some approaches use sum?\n",
    "# output_concat = tf.concat(2, outputs) - we have to reverse and concat\n",
    "output_bw_reversed = tf.reverse(output_bw,dims=(False, False, True))\n",
    "outputs_rev = [output_fw, output_bw_reversed]\n",
    "\n",
    "output_concat = tf.concat(2, outputs_rev)\n",
    "\n",
    "# Calculate logits and probs\n",
    "# Reshape so we can calculate them all at once\n",
    "bi_output_concat_flat = tf.reshape(output_concat, [-1, 2 * hidden_size])\n",
    "# bi_output_concat_flat_clipped = tf.clip_by_value(bi_output_concat_flat, -1., 1.)\n",
    "\n",
    "logits_flat = tf.batch_matmul(bi_output_concat_flat, weights[\"out\"]) + biases[\"out\"]\n",
    "# logits_flat = tf.clip_by_value(logits_flat, -1., 1.)\n",
    "probs_flat = tf.nn.softmax(logits_flat)\n",
    "\n",
    "print \"outputs[-1].shape:%s\" % outputs[-1]#\n",
    "print \"weights[\\\"out\\\"]].shape:%s\" % weights[\"out\"]#.shape\n",
    "print \"biases[\\\"out\\\"].shape:%s\" % biases[\"out\"]#.shape\n",
    "\n",
    "# print pred\n",
    "\n",
    "#make y flat so it match pred shape\n",
    "input_y_flat =  tf.reshape(input_y, [-1])\n",
    "print \"input_y_flat:%s\" % input_y_flat\n",
    "# # Define loss and optimizer\n",
    "\n",
    "def tf_nan_to_zeros_float64(tensor):\n",
    "    return tf.select(tf.is_nan(tensor), tf.zeros(tf.shape(tensor), dtype=tf.float64),tensor)\n",
    "# sparse softmax cros entropy - aka - calculates on non-onehot y!\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_flat, input_y_flat) \n",
    "# replace nans with zeros\n",
    "losses = tf_nan_to_zeros_float64(losses)\n",
    "\n",
    "learn_rate = tf.Variable(learning_rate, trainable=False)\n",
    "\n",
    "#Applying compute and apply gradients\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learn_rate)\n",
    "\n",
    "gvs = optimizer.compute_gradients(losses)\n",
    "print \"gradients:\"\n",
    "for grad, var in gvs:\n",
    "    print \"%s - %s\" % (grad.name, var)\n",
    "    \n",
    "capped_gvs = [(tf.clip_by_value(tf_nan_to_zeros_float64(grad), -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "apply_grads_op=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "## Calculate the accuracy\n",
    "## Used during training\n",
    "# Mask the losses - padded values are zeros\n",
    "mask = tf.sign(tf.cast(input_y_flat, dtype=tf.float64))\n",
    "print \"mask:%s\" % mask\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Bring back to [batch, class_num] shape\n",
    "masked_losses = tf.reshape(masked_losses,  tf.shape(input_y))\n",
    "\n",
    "input_seq_len_float = tf.cast(input_seq_len, dtype=tf.float64)\n",
    "\n",
    "# Calculate mean loss - depending on the dynamic number of elements\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=1) / input_seq_len_float\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "# # Evaluate model\n",
    "print \"probs_flat:%s\" % probs_flat\n",
    "print \"input_y:%s\" % input_y\n",
    "\n",
    "preds_flat = tf.argmax(probs_flat,1)\n",
    "\n",
    "preds_non_paddings = tf.gather(preds_flat, tf.where(tf.greater(input_y_flat, [0])))\n",
    "input_y_non_paddings = tf.gather(input_y_flat, tf.where(tf.greater(input_y_flat, [0])))\n",
    "\n",
    "preds_y = tf.reshape(preds_flat, tf.shape(input_y))\n",
    "\n",
    "correct_pred = tf.equal(preds_non_paddings, input_y_non_paddings)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float64))\n",
    "\n",
    "print \"The model might be okay :D\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "# embeddings_random = 0.01*np.random.uniform(size=(embeddings_number, embedding_size))\n",
    "embeddings_random = vocab_and_embeddings[\"embeddings\"]\n",
    "# embeddings_random[0][0] = float('nan')\n",
    "\n",
    "nan_vals = 0\n",
    "\n",
    "print np.isnan(np.sum(embeddings_random))\n",
    "# for i in range(len(embeddings_random)):\n",
    "#     for j in range(embeddings_random.shape[1]):\n",
    "#         if np.isNan(embeddings_random[i][j]):\n",
    "#             nan_vals +=1\n",
    "# print nan_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0, 0, 0, 0, 0]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "def pad(seq, pad_value, to_size):\n",
    "    pad_seq = []\n",
    "    if len(seq) > to_size:\n",
    "        pad_seq = seq[:to_size]\n",
    "    else:\n",
    "        pad_seq = seq[:] + [pad_value]*(to_size - len(seq))\n",
    "        \n",
    "    return pad_seq\n",
    "\n",
    "print pad([1,2,3], 0, 8)\n",
    "print pad([1,2,3,4,5,6,7,8], 0, 8)\n",
    "print pad([1,2,3,4,5,6,7,8,9,10], 0, 8)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: u'O', 2: u'B-EVENT', 3: u'I-EVENT'}\n",
      "None       (0) accuracy = 1.0\n",
      "O          (1) accuracy = 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix([0, 0, 1, 1], [0,0,1,0])\n",
    "classes_dict = dict([(v,k) for (k,v) in train_seq.y_dict.items()])\n",
    "print classes_dict\n",
    "\n",
    "def print_acc_from_conf_matrix(conf_matrix, classes_dict):\n",
    "    for lblid in range(conf_matrix.shape[0]):\n",
    "        overl_cnt = np.sum(conf_matrix[lblid])\n",
    "        true_cnt = conf_matrix[lblid][lblid]\n",
    "        lbl_acc = float(true_cnt)/float(overl_cnt) if overl_cnt > 0 else 1\n",
    "        lbl_name = classes_dict[lblid] if lblid in classes_dict else \"None\"\n",
    "        print \"%s (%s) accuracy = %s\"%(lbl_name.ljust(10), lblid, lbl_acc)\n",
    "\n",
    "print_acc_from_conf_matrix(conf_matrix, classes_dict)\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f9c96817710>> ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches:251\n",
      "Start training in 1 epochs\n",
      "Epoch 1:  train epoch time 157.982012033 \n",
      "Dev set:\n",
      "Confusion matrix:\n",
      "[[     0      0      0      0]\n",
      " [     2 164676    865      0]\n",
      " [     2   2936   3351      0]\n",
      " [     0    269     77      0]]\n",
      "Accuracy by class:\n",
      "None       (0) accuracy = 1\n",
      "O          (1) accuracy = 0.994762690056\n",
      "B-EVENT    (2) accuracy = 0.53283510892\n",
      "I-EVENT    (3) accuracy = 0.0\n",
      " Dev cost 0.08 | Dev acc 0.98 % in 196.893202066\n",
      "\n",
      "Test set:\n",
      "Confusion matrix:\n",
      "[[    0     0     0]\n",
      " [    2 91493   291]\n",
      " [    1  3648  1979]]\n",
      "Accuracy by class:\n",
      "None       (0) accuracy = 1\n",
      "O          (1) accuracy = 0.996807792038\n",
      "B-EVENT    (2) accuracy = 0.351634683724\n",
      " Test cost 0.16 | Dev acc 0.96 % in 308.662307024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time as ti\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init, feed_dict = None)\n",
    "\n",
    "def prepare_batch_data(data, pad_value=0):\n",
    "    batch_data_seqlens = [len(a.x) for a in data]\n",
    "    max_len = max(batch_data_seqlens)\n",
    "\n",
    "#     batch_data_padded_x = [pad(a.x.tolist(), pad_value, max_len) for a in data]\n",
    "#     batch_data_padded_y = [pad(a.y.tolist(), pad_value, max_len) for a in data]\n",
    "    \n",
    "    batch_data_padded_x = [pad(a.x, pad_value, max_len) for a in data]\n",
    "    batch_data_padded_y = [pad(a.y, pad_value, max_len) for a in data]\n",
    "    # batch_data_padded_x_embedd = [[pad(a.x.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "    # batch_data_padded_y_mask = [[1 if item>0 else 0 for item in pad(a.y.tolist(), pad_value, max_len)] for a in batch_data]\n",
    "\n",
    "    return batch_data_padded_x, batch_data_padded_y, batch_data_seqlens\n",
    "\n",
    "def train_step(x_batch, y_batch, x_batch_seq_len):\n",
    "\n",
    "    feed_dict = {\n",
    "        embeddings_placeholder : embeddings_random,\n",
    "        input_x : x_batch, # batch_data_padded_x,\n",
    "        input_y : y_batch, # batch_data_padded_y,\n",
    "        input_seq_len: x_batch_seq_len # batch_data_seqlens\n",
    "    }\n",
    "\n",
    "    #_, \\\n",
    "    _,\\\n",
    "    res_cost, \\\n",
    "    res_acc, \\\n",
    "    res_output_y,\\\n",
    "    res_mean_loss_by_example, \\\n",
    "    res_logits_flat,\\\n",
    "    res_losses, \\\n",
    "    res_probs_flat, \\\n",
    "    res_input_seq_len_float, \\\n",
    "    res_input_y_flat, res_bi_output_concat_flat, \\\n",
    "    res_output_fw, res_output_bw_reversed, res_embedded_chars, \\\n",
    "    res_weights_out, res_biases_out, \\\n",
    "    res_capped_gvs, \\\n",
    "    res_gvs \\\n",
    "    = sess.run([\n",
    "                apply_grads_op,\n",
    "                mean_loss, \n",
    "                accuracy, \n",
    "                preds_y, \n",
    "                mean_loss_by_example, \n",
    "                logits_flat,\n",
    "                losses, \n",
    "                probs_flat,\n",
    "                input_seq_len_float,\n",
    "                input_y_flat, bi_output_concat_flat,\n",
    "                output_fw, output_bw_reversed, embedded_chars,\n",
    "                weights[\"out\"], biases[\"out\"],\n",
    "                capped_gvs,\n",
    "                gvs], \n",
    "               feed_dict=feed_dict)\n",
    "\n",
    "    check_for_nans = True\n",
    "\n",
    "    \n",
    "    non_zero_input = [oy for oy in y_batch if 2 in oy]\n",
    "    # print \"Non_zero_input:%s\" % len(non_zero_input)\n",
    "    non_zero_preds = [oy for oy in res_output_y if 2 in oy]\n",
    "    # print \"Non-zero preds:%s\" % len(non_zero_preds)\n",
    "    \n",
    "    # print non_zero_preds[:3]\n",
    "    if check_for_nans:\n",
    "        # print \"res_input_seq_len_float:%s\" % res_input_seq_len_float\n",
    "        has_nans = False\n",
    "        # res_output_concat =  sess.run(output_concat, feed_dict=feed_dict)\n",
    "        # print res_output_concat.shape\n",
    "        if np.isnan(np.sum(res_embedded_chars)):\n",
    "            has_nans = True\n",
    "            print \"[res_embedded_chars] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_output_fw)):\n",
    "            has_nans = True\n",
    "            print \"[res_output_fw] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_output_bw_reversed)):\n",
    "            has_nans = True\n",
    "            print \"[res_output_bw_reversed] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_bi_output_concat_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_bi_output_concat_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_weights_out)):\n",
    "            has_nans = True\n",
    "            print \"[res_weights_out] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_biases_out)):\n",
    "            has_nans = True\n",
    "            print \"[res_biases_out] has nans!\"            \n",
    "\n",
    "        if np.isnan(np.sum(res_logits_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_logits_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_probs_flat)):\n",
    "            has_nans = True\n",
    "            print \"[res_probs_flat] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_losses)):\n",
    "            has_nans = True\n",
    "            print \"[res_losses] has nans!\"\n",
    "\n",
    "        if np.isnan(np.sum(res_mean_loss_by_example)):\n",
    "            has_nans = True\n",
    "            print \"[res_mean_loss_by_example] has nans!\"\n",
    "\n",
    "        if np.isnan(res_cost):\n",
    "            has_nans = True\n",
    "            print \"[res_cost] has nans!\"\n",
    "\n",
    "#         if np.isnan(res_gvs):\n",
    "#             has_nans = True\n",
    "#             print \"[res_gvs] has nans!\"\n",
    "\n",
    "#         if np.isnan(res_capped_gvs):\n",
    "#             has_nans = True\n",
    "#             print \"[res_capped_gvs] has nans!\"\n",
    "\n",
    "        if has_nans:\n",
    "            print \"res_weights_out:%s\" % res_weights_out[:5]\n",
    "            print \"res_biases_out:%s\" % res_biases_out\n",
    "            print \"batch_data_padded_x:%s\" % batch_data_padded_x[0]\n",
    "            print \"res_embedded_chars:%s\" % len(res_embedded_chars), len(res_embedded_chars[0])\n",
    "            print \"res_output_fw:%s\" % res_output_fw[0]\n",
    "            print \"res_output_bw_reversed:%s\" % res_output_bw_reversed[0]\n",
    "            # print \"res_bi_output_concat_flat:%s\" % res_bi_output_concat_flat\n",
    "            print \"res_input_y_flat:%s\" % res_input_y_flat\n",
    "            print \"res_logits_flat:%s\" % res_logits_flat\n",
    "            print \"res_res_probs_flat:%s\" % res_probs_flat\n",
    "            print \"res_mean_loss_by_example:%s\" % res_mean_loss_by_example\n",
    "            print \"res_losses:%s\" % res_losses\n",
    "            print \"res_capped_gvs:%s\" % res_capped_gvs\n",
    "            print \"res_gvs:%s\" % res_gvs\n",
    "\n",
    "            # print losses by example\n",
    "            res_losses_reshaped = res_losses.reshape([50, res_losses.shape[0]/50])\n",
    "            for i in range(len(res_losses_reshaped)):\n",
    "                if np.isnan(np.sum(res_losses_reshaped[i])):\n",
    "                    print res_losses_reshaped[i]\n",
    "            raise Exception(\"Fucking NANS!\")\n",
    "\n",
    "    return res_cost, res_acc\n",
    "        \n",
    "def dev_step(x_batch, y_batch, x_batch_seq_len):\n",
    "    \n",
    "    feed_dict = {\n",
    "        embeddings_placeholder : embeddings_random,\n",
    "        input_x : x_batch, # batch_data_padded_x,\n",
    "        input_y : y_batch, # batch_data_padded_y,\n",
    "        input_seq_len: x_batch_seq_len # batch_data_seqlens\n",
    "    }\n",
    "\n",
    "    # _ ,\\\n",
    "    res_cost, \\\n",
    "    res_acc, \\\n",
    "    res_output_y,\\\n",
    "    res_mean_loss_by_example, \\\n",
    "    res_logits_flat,\\\n",
    "    res_losses, \\\n",
    "    res_probs_flat, \\\n",
    "    res_input_seq_len_float, \\\n",
    "    res_input_y_flat, res_bi_output_concat_flat, \\\n",
    "    res_output_fw, res_output_bw_reversed, res_embedded_chars, \\\n",
    "    res_weights_out, res_biases_out, \\\n",
    "    res_capped_gvs, \\\n",
    "    res_gvs \\\n",
    "    = sess.run([\n",
    "                # apply_grads_op, - commented so does not apply gradients\n",
    "                mean_loss, \n",
    "                accuracy, \n",
    "                preds_y,\n",
    "                mean_loss_by_example, \n",
    "                logits_flat,\n",
    "                losses, \n",
    "                probs_flat,\n",
    "                input_seq_len_float,\n",
    "                input_y_flat, bi_output_concat_flat,\n",
    "                output_fw, output_bw_reversed, embedded_chars,\n",
    "                weights[\"out\"], biases[\"out\"],\n",
    "                capped_gvs,\n",
    "                gvs\n",
    "                ], \n",
    "               feed_dict=feed_dict)\n",
    "\n",
    "    # print \" Dev cost %2.2f | Dev batch acc %2.2f %% in %s\\n\" % (res_cost, res_acc, ti.time()-start),\n",
    "\n",
    "    return res_output_y, res_cost, res_acc\n",
    "\n",
    "pad_value = 0\n",
    "train_batches_cnt = len(train_seq)/batch_size\n",
    "dev_batches_cnt = len(dev_seq)/batch_size\n",
    "test_batches_cnt = len(test_seq)/batch_size\n",
    "\n",
    "print \"Batches:%s\" % train_batches_cnt\n",
    "#batches_cnt = min(batches_cnt, 1)\n",
    "epochs_cnt = 1\n",
    "#train with batches\n",
    "max_len = 0\n",
    "print \"Start training in %s epochs\" % epochs_cnt\n",
    "for epoch in range(0, epochs_cnt):\n",
    "    print \"Epoch %d:\" % (epoch+1),\n",
    "    start_epoch = ti.time()\n",
    "    \n",
    "    for i in range(0, train_batches_cnt):\n",
    "        # print\"Batch %s =================\" % (i+1)\n",
    "        \n",
    "        #Get the batch\n",
    "        batch_data = train_seq[i*batch_size:i*batch_size+batch_size]\n",
    "        batch_data_padded_x, batch_data_padded_y, batch_data_seqlens = prepare_batch_data(data=batch_data)\n",
    "\n",
    "        #Do the train step\n",
    "        start = ti.time()\n",
    "        cost, acc = train_step(x_batch = batch_data_padded_x, y_batch = batch_data_padded_y, x_batch_seq_len = batch_data_seqlens)\n",
    "        # print \" Train cost %2.2f | Train batch acc %2.2f %% in %s\\n\" % (cost, acc, ti.time()-start)\n",
    "        \n",
    "    print \" train epoch time %s \" % (ti.time()-start_epoch)\n",
    "    \n",
    "    #Dev eval - once per epoch\n",
    "    print \"Dev set:\"\n",
    "    input_y_all = []\n",
    "    pred_y_all = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, dev_batches_cnt):\n",
    "        batch_data = dev_seq[i*batch_size:i*batch_size+batch_size]\n",
    "        batch_data_padded_x, batch_data_padded_y, batch_data_seqlens = prepare_batch_data(data=batch_data)\n",
    "    \n",
    "        res_pred_y, cost, acc = dev_step(x_batch = batch_data_padded_x, y_batch = batch_data_padded_y, x_batch_seq_len = batch_data_seqlens)\n",
    "        \n",
    "#         print batch_data_padded_y[10]\n",
    "#         print res_pred_y[10]\n",
    "        \n",
    "        for j in range(0, batch_size):\n",
    "            input_y_all.extend(batch_data_padded_y[j][:batch_data_seqlens[j]])\n",
    "            pred_y_all.extend(res_pred_y[j][:batch_data_seqlens[j]])\n",
    "            \n",
    "            \n",
    "    # eval\n",
    "    print \"Confusion matrix:\"\n",
    "    conf_matrix = confusion_matrix(input_y_all, pred_y_all)\n",
    "    print conf_matrix\n",
    "    print \"Accuracy by class:\"\n",
    "    print_acc_from_conf_matrix(conf_matrix, classes_dict)\n",
    "    \n",
    "#     print \"Class  : P, R, F-Score:\" \n",
    "#     print \"B-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=2)\n",
    "#     print \"I-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=3)\n",
    "#     \n",
    "    print \" Dev cost %2.2f | Dev acc %2.2f %% in %s\\n\" % (cost, acc, ti.time()-start)\n",
    "    \n",
    "    #################\n",
    "    ### Test eval ###\n",
    "    #################\n",
    "    print \"Test set:\"\n",
    "    input_y_all = []\n",
    "    pred_y_all = []\n",
    "    pred_y_all_varlength = []\n",
    "    \n",
    "    for i in range(0,test_batches_cnt):\n",
    "        batch_data = test_seq[i*batch_size:i*batch_size+batch_size]\n",
    "        batch_data_padded_x, batch_data_padded_y, batch_data_seqlens = prepare_batch_data(data=batch_data)\n",
    "    \n",
    "        res_pred_y, cost, acc = dev_step(x_batch = batch_data_padded_x, y_batch = batch_data_padded_y, x_batch_seq_len = batch_data_seqlens)\n",
    "        \n",
    "#         print batch_data_padded_y[10]\n",
    "#         print res_pred_y[10]\n",
    "        \n",
    "        for j in range(0, batch_size):\n",
    "            input_y_all.extend(batch_data_padded_y[j][:batch_data_seqlens[j]])\n",
    "            pred_y_all.extend(res_pred_y[j][:batch_data_seqlens[j]])\n",
    "            \n",
    "            #output to be formatted\n",
    "            pred_y_all_varlength.append(res_pred_y[j][:batch_data_seqlens[j]])\n",
    "            \n",
    "            \n",
    "    # eval\n",
    "    print \"Confusion matrix:\"\n",
    "    conf_matrix = confusion_matrix(input_y_all, pred_y_all)\n",
    "    print conf_matrix\n",
    "    print \"Accuracy by class:\"\n",
    "    print_acc_from_conf_matrix(conf_matrix, classes_dict)\n",
    "    \n",
    "#     print \"Class  : P, R, F-Score:\" \n",
    "#     print \"B-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=2)\n",
    "#     print \"I-Event:\"\n",
    "#     print precision_recall_fscore_support(input_y_all, pred_y_all, average=None, pos_label=3)\n",
    "#     \n",
    "    print \" Test cost %2.2f | Dev acc %2.2f %% in %s\\n\" % (cost, acc, ti.time()-start)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract event nuggets from labeled sentences\n",
    "def extract_eventnuggets(data_meta, use_per_doc_event_indexing=True):\n",
    "    docs_with_events = []\n",
    "    \n",
    "    # metadata fields:\n",
    "    # tokens\n",
    "    # deps_basic\n",
    "    # lemmas\n",
    "    # deps_cc\n",
    "    # pos\n",
    "    # parse\n",
    "    # file_name\n",
    "    # file_id\n",
    "    # char_offsets\n",
    "    # labels_realis\n",
    "    # labels_type_full\n",
    "    # labels_event\n",
    "\n",
    "    curr_doc_id = \"\"\n",
    "    event_id = 0\n",
    "    \n",
    "    curr_doc_data = {}\n",
    "    for data_item in data_meta:\n",
    "        doc_id = data_item[\"file_id\"]\n",
    "        if doc_id <> curr_doc_id:\n",
    "            if curr_doc_id<>\"\":\n",
    "                docs_with_events.append(curr_doc_data)\n",
    "                \n",
    "            # start new doc data export\n",
    "            curr_doc_id = doc_id\n",
    "            \n",
    "            curr_doc_data = {}\n",
    "            curr_doc_data[\"doc_id\"] = curr_doc_id\n",
    "            curr_doc_data[\"file_id\"] = curr_doc_id\n",
    "            curr_doc_data[\"file_name\"] = data_item[\"file_name\"]\n",
    "            \n",
    "            curr_doc_data[\"event_nuggets\"] = []\n",
    "            \n",
    "        lbl_event_bevent = u\"B-EVENT\"\n",
    "        lbl_event_ievent = u\"I-EVENT\"\n",
    "\n",
    "        lbl_bevent_cnt = sum([1 for xx in data_item[\"labels_event\"] if xx == lbl_event_bevent])\n",
    "        # print \"lbl_bevent_cnt:%s\" % lbl_bevent_cnt\n",
    "        lbl_ievent_cnt = sum([1 for xx in data_item[\"labels_event\"] if xx == lbl_event_ievent])\n",
    "        # print \"lbl_ievent_cnt:%s\" % lbl_ievent_cnt\n",
    "\n",
    "        curr_event_token_i = 0\n",
    "        sent_len = len(data_item[\"tokens\"])\n",
    "        for i in range(0, sent_len):\n",
    "            if data_item[\"labels_event\"][i] == lbl_event_bevent:\n",
    "                \n",
    "                event_id += 1\n",
    "                curr_event_token_i = i \n",
    "                curr_event_nugget = {}\n",
    "                curr_event_nugget[\"event_id\"]=\"E%s\"%event_id\n",
    "\n",
    "                #max search for I-EVENT for longer span events\n",
    "                max_span_tokens_search = 5 # In the data there is no span longer the 4 - we set 5 for convenience. \n",
    "                                           # Howerver longer would span would be an error.\n",
    "                last_ievent_idx = 0\n",
    "                if lbl_ievent_cnt>0:\n",
    "                    for j in range(i+1, min(i + max_span_tokens_search, sent_len-1)):\n",
    "                        if data_item[\"labels_event\"][j] == lbl_event_ievent:\n",
    "                            last_ievent_idx = j\n",
    "                            lbl_ievent_cnt -= 1\n",
    "\n",
    "                        elif data_item[\"labels_event\"] == lbl_event_bevent:\n",
    "                            break\n",
    "\n",
    "                if last_ievent_idx == 0:\n",
    "                    last_ievent_idx = curr_event_token_i\n",
    "\n",
    "                nugget_span = (100000,0)\n",
    "                nugget_txt = \"\"\n",
    "\n",
    "                curr_event_nugget[\"tokens\"] = []\n",
    "                curr_event_nugget[\"char_offsets\"] = []\n",
    "                range_curr = range(curr_event_token_i, last_ievent_idx+1)\n",
    "                \n",
    "                # print range_curr\n",
    "                for k in range_curr:\n",
    "                    token = data_item[\"tokens\"][k]\n",
    "                    curr_event_nugget[\"tokens\"].append(token)\n",
    "\n",
    "                    char_offset = data_item[\"char_offsets\"][k]\n",
    "                    curr_event_nugget[\"char_offsets\"].append(char_offset)\n",
    "\n",
    "                    prev_token_span_to = nugget_span[1]\n",
    "                    # add spaces if tokens are not connected to each other \"I\"<s>\"eat\"<s>\"ham\"\n",
    "                    if prev_token_span_to > 0 and  (char_offset[1] - prev_token_span_to - 1) > 0:\n",
    "                        nugget_txt += \" \" * (char_offset[0] - prev_token_span_to)\n",
    "\n",
    "                    nugget_txt += token\n",
    "\n",
    "                    nugget_span = (min(nugget_span[0], char_offset[0]), char_offset[1])\n",
    "\n",
    "                curr_event_nugget[\"text\"] = nugget_txt\n",
    "                curr_event_nugget[\"span\"] = nugget_span\n",
    "                curr_event_nugget[\"realis\"] = data_item[\"labels_realis\"][i][2:] # Assign the value of B-EVENT\n",
    "                curr_event_nugget[\"type_full\"] = data_item[\"labels_type_full\"][i][2:] # Assign the value of B-EVENT\n",
    "                \n",
    "                #Confidence score\n",
    "                curr_event_nugget[\"span_confidence\"] = 1.0\n",
    "                curr_event_nugget[\"realis_confidence\"] = 1.0\n",
    "                curr_event_nugget[\"type_confidence\"] = 1.0\n",
    "                \n",
    "                # print \"E%s - %s\" %(event_id,curr_event_nugget)\n",
    "                curr_doc_data[\"event_nuggets\"].append(curr_event_nugget)\n",
    "        \n",
    "    # add the last document\n",
    "    docs_with_events.append(curr_doc_data)\n",
    "     \n",
    "                        \n",
    "                        \n",
    "\n",
    "    return docs_with_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': u'1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn.txt', 'doc_id': u'1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn', 'file_id': u'1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn', 'event_nuggets': [{'span': (205, 228), 'event_id': 'E1', 'text': u'handed maximum sentence', 'span_confidence': 1.0, 'type_full': u'Justice_Sentence', 'tokens': [u'handed', u'maximum', u'sentence'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[205, 211], [212, 219], [220, 228]], 'realis': u'Actual'}, {'span': (241, 250), 'event_id': 'E2', 'text': u'sentences', 'span_confidence': 1.0, 'type_full': u'Justice_Sentence', 'tokens': [u'sentences'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[241, 250]], 'realis': u'Actual'}, {'span': (291, 297), 'event_id': 'E3', 'text': u'prison', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'prison'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[291, 297]], 'realis': u'Generic'}, {'span': (1274, 1286), 'event_id': 'E4', 'text': u'make a claim', 'span_confidence': 1.0, 'type_full': u'Contact_Communicate', 'tokens': [u'make', u'a', u'claim'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[1274, 1278], [1279, 1280], [1281, 1286]], 'realis': u'Actual'}, {'span': (1303, 1311), 'event_id': 'E5', 'text': u'arrested', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrested'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[1303, 1311]], 'realis': u'Generic'}, {'span': (1322, 1332), 'event_id': 'E6', 'text': u'imprisoned', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'imprisoned'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[1322, 1332]], 'realis': u'Generic'}, {'span': (1354, 1362), 'event_id': 'E7', 'text': u'arrested', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrested'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[1354, 1362]], 'realis': u'Actual'}, {'span': (1386, 1396), 'event_id': 'E8', 'text': u'extradited', 'span_confidence': 1.0, 'type_full': u'Justice_Extradite', 'tokens': [u'extradited'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[1386, 1396]], 'realis': u'Actual'}, {'span': (2474, 2486), 'event_id': 'E9', 'text': u'make a claim', 'span_confidence': 1.0, 'type_full': u'Contact_Communicate', 'tokens': [u'make', u'a', u'claim'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[2474, 2478], [2479, 2480], [2481, 2486]], 'realis': u'Actual'}, {'span': (2503, 2511), 'event_id': 'E10', 'text': u'arrested', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrested'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[2503, 2511]], 'realis': u'Generic'}, {'span': (2522, 2532), 'event_id': 'E11', 'text': u'imprisoned', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'imprisoned'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[2522, 2532]], 'realis': u'Generic'}, {'span': (2554, 2562), 'event_id': 'E12', 'text': u'arrested', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrested'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[2554, 2562]], 'realis': u'Actual'}, {'span': (2586, 2596), 'event_id': 'E13', 'text': u'extradited', 'span_confidence': 1.0, 'type_full': u'Justice_Extradite', 'tokens': [u'extradited'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[2586, 2596]], 'realis': u'Actual'}, {'span': (5151, 5168), 'event_id': 'E14', 'text': u'ship military aid', 'span_confidence': 1.0, 'type_full': u'Transaction_Transfer-Ownership', 'tokens': [u'ship', u'military', u'aid'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5151, 5155], [5156, 5164], [5165, 5168]], 'realis': u'Other'}, {'span': (5187, 5193), 'event_id': 'E15', 'text': u'demand', 'span_confidence': 1.0, 'type_full': u'Contact_Communicate', 'tokens': [u'demand'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5187, 5193]], 'realis': u'Other'}, {'span': (5241, 5247), 'event_id': 'E16', 'text': u'arrest', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrest'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5241, 5247]], 'realis': u'Other'}, {'span': (5255, 5264), 'event_id': 'E17', 'text': u'extradite', 'span_confidence': 1.0, 'type_full': u'Justice_Extradite', 'tokens': [u'extradite'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5255, 5264]], 'realis': u'Other'}, {'span': (5284, 5290), 'event_id': 'E18', 'text': u'prison', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'prison'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5284, 5290]], 'realis': u'Other'}, {'span': (5784, 5801), 'event_id': 'E19', 'text': u'ship military aid', 'span_confidence': 1.0, 'type_full': u'Transaction_Transfer-Ownership', 'tokens': [u'ship', u'military', u'aid'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5784, 5788], [5789, 5797], [5798, 5801]], 'realis': u'Other'}, {'span': (5820, 5826), 'event_id': 'E20', 'text': u'demand', 'span_confidence': 1.0, 'type_full': u'Contact_Communicate', 'tokens': [u'demand'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5820, 5826]], 'realis': u'Other'}, {'span': (5874, 5880), 'event_id': 'E21', 'text': u'arrest', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'arrest'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5874, 5880]], 'realis': u'Other'}, {'span': (5888, 5897), 'event_id': 'E22', 'text': u'extradite', 'span_confidence': 1.0, 'type_full': u'Justice_Extradite', 'tokens': [u'extradite'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5888, 5897]], 'realis': u'Other'}, {'span': (5917, 5923), 'event_id': 'E23', 'text': u'prison', 'span_confidence': 1.0, 'type_full': u'Justice_Arrest-Jail', 'tokens': [u'prison'], 'type_confidence': 1.0, 'realis_confidence': 1.0, 'char_offsets': [[5917, 5923]], 'realis': u'Other'}]}]\n"
     ]
    }
   ],
   "source": [
    "data_with_meta = [xx for xx in dev_seq_meta if u\"I-EVENT\" in xx[\"labels_event\"]][:5] # test events with I\n",
    "docs_with_nuggets = extract_eventnuggets(data_meta=data_with_meta, use_per_doc_event_indexing=True)\n",
    "\n",
    "# print data_with_meta \n",
    "print docs_with_nuggets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BeginOfDocument 04bfe2831596d665b1585d8bf7bedd47\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE32\t219\tpardon\tJustice_Pardon\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE73\t254\tcharged\tJustice_Charge-Indict\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE187\t238\texecutions\tJustice_Execute\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE199\t297\tamnesties\tJustice_Pardon\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE248\t44\tsaid\tContact_Broadcast\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE261\t77\tkidnapping\tConflict_Attack\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE289\t194\treports\tContact_Broadcast\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE290\t260\tkidnapping\tConflict_Attack\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE308\t325\tdisappearance\tConflict_Attack\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE325\t53\tcharged\tJustice_Charge-Indict\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE328\t284\tamnesty\tJustice_Pardon\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE344\t234\ttorture\tConflict_Attack\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE430\t327\tkidnapping\tMovement_Transport-Person\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE460\t164\ttaken\tMovement_Transport-Person\tOther\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE464\t213\tAmnesty\tJustice_Pardon\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE598\t204\theld\tJustice_Arrest-Jail\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE625\t60\tretired\tPersonnel_End-Position\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE642\t327\tkidnapping\tConflict_Attack\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE688\t238\texecutions\tLife_Die\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE774\t260\tkidnapping\tMovement_Transport-Person\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE792\t77\tkidnapping\tMovement_Transport-Person\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE839\t201\ttortured\tConflict_Attack\tActual\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE877\t314\tAmnesty\tJustice_Pardon\tGeneric\n",
    "brat_conversion\t04bfe2831596d665b1585d8bf7bedd47\tE999\t142\tdetained\tJustice_Arrest-Jail\tActual\n",
    "@Coreference\tC0\tE73,E325\n",
    "@Coreference\tC1\tE460,E792\n",
    "@Coreference\tC2\tE598,E999\n",
    "@Coreference\tC3\tE877,E32,E328,E464\n",
    "@Coreference\tC4\tE642,E290\n",
    "@Coreference\tC5\tE430,E774\n",
    "#EndOfDocument\n",
    "\n",
    "#BeginOfDocument <file_id>\n",
    "<systen_name>\t<file_id>\t<event_key>\t<span_begin>,<span_end>\t<mention_text>\t<event_type=Justice_Pardon>\t<Realis=Generic>\t<event_span_confidence>\t<event_type_confidence>\t<realis_confidence>\n",
    "@Coreference\t<coref_cluster_name=C1>\t<event_key_1>,<event_key_2>\n",
    "#EndOfDocument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_output_file_conttent(docs_with_nuggets, system_name):\n",
    "    output_str = \"\"\n",
    "    for doc in docs_with_nuggets:\n",
    "        doc_id = doc[\"doc_id\"]\n",
    "        output_str += \"#BeginOfDocument\\t%s\\n\" % doc_id\n",
    "        \n",
    "        for event_nugget in doc[\"event_nuggets\"]:\n",
    "            event_id = event_nugget[\"event_id\"]\n",
    "            mention_text = event_nugget[\"text\"]\n",
    "            span = event_nugget[\"span\"]\n",
    "            realis = event_nugget[\"realis\"]\n",
    "            type_full = event_nugget[\"type_full\"]\n",
    "\n",
    "            #Confidence score\n",
    "            span_confidence = event_nugget[\"span_confidence\"]\n",
    "            realis_confidence = event_nugget[\"realis_confidence\"]\n",
    "            type_confidence = event_nugget[\"type_confidence\"]\n",
    "            \n",
    "            output_str += \"{system_name}\\t{doc_id}\\t{event_id}\\t{span_begin},{span_end}\\t{mention_text}\\t{event_type}\\t{realis}\\t{event_span_confidence}\\t{event_type_confidence}\\t{realis_confidence}\\n\".format(\n",
    "                                    system_name = system_name,\n",
    "                                    doc_id = doc_id,\n",
    "                                    event_id = event_id,\n",
    "                                    span_begin = span[0],\n",
    "                                    span_end = span[1],\n",
    "                                    mention_text = mention_text,\n",
    "                                    event_type = type_full,\n",
    "                                    realis = realis,\n",
    "                                    event_span_confidence = span_confidence,\n",
    "                                    event_type_confidence = realis_confidence,\n",
    "                                    realis_confidence = type_confidence)\n",
    "            \n",
    "        output_str += \"#EndOfDocument\\n\"\n",
    "        \n",
    "        # Coreference\n",
    "        if \"coreference_list\" in doc:\n",
    "            for coref in doc[\"coreference_list\"]:\n",
    "                coref_id = coref[\"coref_id\"]\n",
    "                coref_events_keys = coref[\"events_keys\"]\n",
    "                coref_events_keys_str = string.join(coref_events_keys, \",\")\n",
    "                \n",
    "                output_str += \"@Coreference\\t{coref_id}\\t{events_keys}\\n\".format(coref_id = coref_id, \n",
    "                                        events_keys = coref_events_keys_str)\n",
    "                \n",
    "    \n",
    "    return output_str \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#BeginOfDocument\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE1\t205,228\thanded maximum sentence\tJustice_Sentence\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE2\t241,250\tsentences\tJustice_Sentence\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE3\t291,297\tprison\tJustice_Arrest-Jail\tGeneric\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE4\t1274,1286\tmake a claim\tContact_Communicate\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE5\t1303,1311\tarrested\tJustice_Arrest-Jail\tGeneric\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE6\t1322,1332\timprisoned\tJustice_Arrest-Jail\tGeneric\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE7\t1354,1362\tarrested\tJustice_Arrest-Jail\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE8\t1386,1396\textradited\tJustice_Extradite\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE9\t2474,2486\tmake a claim\tContact_Communicate\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE10\t2503,2511\tarrested\tJustice_Arrest-Jail\tGeneric\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE11\t2522,2532\timprisoned\tJustice_Arrest-Jail\tGeneric\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE12\t2554,2562\tarrested\tJustice_Arrest-Jail\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE13\t2586,2596\textradited\tJustice_Extradite\tActual\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE14\t5151,5168\tship military aid\tTransaction_Transfer-Ownership\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE15\t5187,5193\tdemand\tContact_Communicate\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE16\t5241,5247\tarrest\tJustice_Arrest-Jail\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE17\t5255,5264\textradite\tJustice_Extradite\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE18\t5284,5290\tprison\tJustice_Arrest-Jail\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE19\t5784,5801\tship military aid\tTransaction_Transfer-Ownership\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE20\t5820,5826\tdemand\tContact_Communicate\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE21\t5874,5880\tarrest\tJustice_Arrest-Jail\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE22\t5888,5897\textradite\tJustice_Extradite\tOther\t1.0\t1.0\t1.0\n",
      "aiphes_hd_t16\t1b386c986f9d06fd0a0dda70c3b8ade9.mpdf.tkn\tE23\t5917,5923\tprison\tJustice_Arrest-Jail\tOther\t1.0\t1.0\t1.0\n",
      "#EndOfDocument\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str = generate_output_file_conttent([docs_with_nuggets[0]], system_name=\"aiphes_hd_t16\") \n",
    "print str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
